{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktuser/projects/YaCUP/YaCUP-Music/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=256, problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4179, -0.6047, -0.0948, -0.0974,  0.0524, -0.0569,  0.2535,  0.2742,\n",
       "         -0.7018,  0.2145, -0.1129,  0.3524, -0.3026, -0.0278,  0.1961, -0.2796,\n",
       "          0.1969,  0.1593,  0.0839,  0.3601,  0.0644,  0.1702,  0.3429, -0.2367,\n",
       "          0.0768, -0.1154, -0.0511,  0.2031,  0.3877, -0.4218, -0.4209, -0.2244,\n",
       "          0.3164, -0.0452,  0.4087, -0.3593,  0.2016, -0.0212, -0.4086, -0.4624,\n",
       "         -0.0613,  0.0263,  0.0206,  0.2927, -0.1594,  0.1450, -0.1615,  0.2477,\n",
       "          0.3275,  0.0871,  0.6448, -0.5960, -0.1620, -0.4100, -0.1182,  0.4707,\n",
       "         -0.3884, -0.1833,  0.2390,  0.0340,  0.1669, -0.0933, -0.1295, -0.2809,\n",
       "         -0.0256, -0.2331,  0.3238,  0.0989,  0.1943,  0.0863,  0.2977, -0.5551,\n",
       "          0.2418,  0.0092, -0.2980, -0.2051, -0.2934,  0.5690, -0.3704, -0.1538,\n",
       "         -0.5173,  0.0311, -0.0684,  0.2077, -0.0089, -0.2699, -0.5253,  0.1557,\n",
       "         -0.0649,  0.4046,  0.0069,  0.3059, -0.2447, -0.1999, -0.2108,  0.1310,\n",
       "          0.0512, -0.0582, -0.2436,  0.3583,  0.0705, -0.2211,  0.1097,  0.7385,\n",
       "         -0.0979,  0.0634,  0.2160,  0.0220, -0.1294,  0.4711,  0.2180, -0.2175,\n",
       "          0.1135, -0.1823,  0.2030, -0.0746,  0.0400, -0.1942,  0.9262,  0.3466,\n",
       "          0.2648,  0.1282,  0.0766,  0.1370,  0.1239,  0.0043,  0.1350,  0.2765,\n",
       "         -0.1563, -0.0204,  0.2822, -0.1974,  0.0183,  0.2422,  0.0997, -0.1164,\n",
       "         -0.0051,  0.3383, -0.0227,  0.1773,  0.5043, -0.2042, -0.2857, -0.0724,\n",
       "         -0.0612,  0.0489,  0.2459,  0.0441,  0.0353, -0.3124,  0.2080,  0.3623,\n",
       "         -0.1677, -0.0290, -0.4056, -0.1417, -0.0332,  0.2945,  0.0589, -0.5839,\n",
       "         -0.4509, -0.4201, -0.1493, -0.2930, -0.2439,  0.1051, -0.0416,  0.4243,\n",
       "          0.0427,  0.0250,  0.0461,  0.0661, -0.1272,  0.3249,  0.2761, -0.0587,\n",
       "          0.1758,  0.1649,  0.2075,  0.0085,  0.3439, -0.1361,  0.0824,  0.2049,\n",
       "         -0.5026, -0.0353, -0.3354,  0.0184, -0.2987,  0.1560,  0.5134,  0.5040,\n",
       "         -0.0486,  0.3152, -0.3412,  0.1854,  0.3090, -0.0847, -0.1781, -0.1289,\n",
       "         -0.3144, -0.3566, -0.1367, -0.3635,  0.2318, -0.3712,  0.3509, -0.0688,\n",
       "         -0.0967,  0.0744,  0.2062, -0.1697, -0.1157,  0.1963, -0.2120,  0.0885,\n",
       "          0.3460, -0.1339,  0.3809,  0.2006,  0.3365, -0.1340,  0.0068, -0.0558,\n",
       "          0.4989,  0.6330,  0.4422,  0.2877,  0.1835, -0.2155,  0.0757,  0.3787,\n",
       "         -0.0219, -0.1111, -0.3093,  0.1138,  0.1238,  0.2293,  0.4317,  0.0608,\n",
       "         -0.3343,  0.2436, -0.2362,  0.1443, -0.3694,  0.2021, -0.3850,  0.2878,\n",
       "          0.0698, -0.6769,  0.2345,  0.3278,  0.1373, -0.1045, -0.2516,  0.2460]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.zeros((1, 4, 768))\n",
    "\n",
    "out = model(inputs_embeds=embeddings)\n",
    "\n",
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import math\n",
    "from typing import List\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        d_model = 768\n",
    "        nhead = 2\n",
    "        num_layers = 2\n",
    "        dropout = 0.5\n",
    "        n_classes = 256\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.query_embed = nn.Embedding(n_classes, d_model)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "    def forward(self, embeddings: List[Tensor]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "\n",
    "        out_batch = []\n",
    "\n",
    "        for sample_emb in embeddings:\n",
    "            sample_emb = sample_emb.unsqueeze(0)\n",
    "            sample_emb = self.pos_encoder(sample_emb)\n",
    "            output = self.transformer_decoder(self.query_embed.weight.unsqueeze(0), sample_emb)\n",
    "            output = self.linear(output).view(1, -1)\n",
    "\n",
    "            out_batch.append( output )\n",
    "\n",
    "        out_batch = torch.cat(out_batch, dim=0)\n",
    "\n",
    "        return out_batch\n",
    "    \n",
    "\n",
    "device = torch.device(0)\n",
    "model = TransformerDecoder().to(device)\n",
    "embeddings = [torch.rand(8, 768, device=device), torch.rand(3, 768, device=device)]\n",
    "\n",
    "out = model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 512])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 256, 512])\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from transformers import WhisperModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Whisper(nn.Module):\n",
    "    def __init__(self, freeze) -> None:\n",
    "        super().__init__()\n",
    "        self.model = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.embed_proj = nn.Linear(768, 512)\n",
    "        self.pos_encoder = PositionalEncoding(512, 0.1)\n",
    "        self.decoder_inputs_embeds = nn.Embedding(256, 512)\n",
    "        self.output_proj = nn.Linear(512, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embed_proj.bias.data.zero_()\n",
    "        self.embed_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        self.output_proj.bias.data.zero_()\n",
    "        self.output_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        out_batch = []\n",
    "\n",
    "        for sample_emb in embeddings:\n",
    "            sample_emb = sample_emb.unsqueeze(0)\n",
    "            sample_emb = self.embed_proj(sample_emb)\n",
    "            sample_emb = self.pos_encoder(sample_emb)\n",
    "            output = self.model(encoder_outputs=(sample_emb,), decoder_inputs_embeds=self.decoder_inputs_embeds.weight.unsqueeze(0)).last_hidden_state\n",
    "            output = self.output_proj(output).view(1, -1)\n",
    "\n",
    "            out_batch.append( output )\n",
    "\n",
    "        out_batch = torch.cat(out_batch, dim=0)\n",
    "\n",
    "        return out_batch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = Whisper(True).to(device)\n",
    "\n",
    "embeddings = [torch.rand(8, 768, device=device), torch.rand(3, 768, device=device)]\n",
    "out = model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = torch.rand(1, 8, 512, device=device)\n",
    "decoder_inputs_embeds = torch.rand(1, 256, 512)\n",
    "\n",
    "output = model(encoder_outputs=(embeds,), decoder_inputs_embeds=decoder_inputs_embeds)\n",
    "\n",
    "output.last_hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
